\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}

% ---------- Listing setup (for YAML/JSON) ----------
\lstdefinelanguage{YAML}{
  keywords={true,false,null,y,n},
  keywordstyle=\color{blue}\bfseries,
  basicstyle=\ttfamily\small,
  comment=[l]{\#},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  sensitive=true
}

% ---------- Theorem environments ----------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

% ---------- Math macros ----------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathds{1}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\cvar}{\mathrm{CVaR}}
\newcommand{\VaR}{\mathrm{VaR}}
\newcommand{\grad}{\nabla}
\newcommand{\defeq}{\vcentcolon=}

\title{\bf Temperament-Conditioned Policy Search (TCPS):\\
Personality Priors for Sample-Efficient and Risk-Aware Control}
\author{Anonymous}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We encode operator temperament as a measurable prior over policies or policy parameters and optimize a single entropy-regularized control objective with safety constraints. By shrinking the effective hypothesis class toward behavior families consistent with lived play, TCPS reduces label complexity and tail-risk violations while preserving escape routes under mis-specification. We formalize two equivalent views---a behavior-prior view and a parameter-prior (PAC-Bayesian) view---and propose a tempered, log-linear compositional prior over policy parameters to compose traits without brittle over-confidence. Empirically, in safety-sensitive social-control domains with regime targets, TCPS attains target sets in fewer episodes and with lower risk than strong behavior-regularized and imitation baselines, and transfers more gracefully under hostile perturbations.
\end{abstract}

\vspace{-0.5em}
\section{Introduction}
Human operators carry stable temperament biases: aversion to bystander harm, willingness to sustain retaliation costs, or preferences for face-restoring actions. Standard RL ignores these priors or learns them slowly from scratch, spending samples on implausible behavior. TCPS injects temperament as a prior into entropy-regularized control to bias exploration toward realistic policy families, lowering sample complexity and improving safety.

We (i) formalize temperament as either a default policy $\pi_0^\tau$ or an exponential-family parameter prior $p_\tau(\theta)$; (ii) show how to compose trait factors with temperature to avoid brittle products; (iii) provide a KL-budget ``safety valve'' that guarantees escape from bad priors; and (iv) evaluate TCPS against strong baselines with de-confounded ablations.

\paragraph{Contributions.}
\begin{enumerate}[leftmargin=1.5em,itemsep=2pt,topsep=2pt]
\item A unified objective for TCPS with two operationally equivalent views: behavior-prior KL regularization and parameter-prior PAC-Bayesian regularization.
\item A tempered log-linear (exponential-family) prior to compose trait experts additively, with sparsity to prevent trait crowding.
\item A mis-specification escape mechanism via a dual-updated KL budget and temperature scheduling.
\item A principled evaluation: time-to-target (regime) CDFs, CVaR on safety costs, probability of budget breach, regret, and KL drift; plus transfer to hostile shifts.
\end{enumerate}

\section{Preliminaries}
\label{sec:prelim}
\paragraph{CMDP.}
We consider a constrained Markov decision process (CMDP) $\mathcal{M}=(\mathcal{S},\mathcal{A},P,r,c,\gamma)$ with state space $\mathcal{S}$, action space $\mathcal{A}$, transitions $P$, reward $r$, safety cost $c\ge 0$, and discount $\gamma\in(0,1)$. Let $\pi_\theta(a|s)$ be a parametric stochastic policy.

\paragraph{Regime targets.}
We model desired operating regimes as target sets $G\subseteq \mathcal{S}$ and reject sets $R\subseteq \mathcal{S}$. Time-to-regime is the first-passage time to $G$. Risk-of-ruin is the probability of ever hitting $R$.

\paragraph{Risk.}
For cumulative cost $Z\defeq\sum_{t=0}^{T} c(s_t,a_t)$, CVaR at level $q\in(0,1)$ is
$\cvar_q(Z)\defeq \inf_{\eta\in\R}\left\{\eta + \frac{1}{q}\E[(Z-\eta)_+]\right\}$.

\section{Temperament as Data}
\label{sec:temperament}
Let $\tau\in\R^k$ denote a temperament embedding obtained by one of three measurable routes:
\begin{enumerate}[leftmargin=1.5em,itemsep=2pt,topsep=2pt]
    \item \textbf{Probe episodes} (few scripted dilemmas) mapped by an encoder $\phi$.
    \item \textbf{Questionnaire} mapped linearly or via a small network.
    \item \textbf{Historical logs} (empirical Bayes).
\end{enumerate}
We select one primary route for the main experiments (probe episodes) and use the others for sensitivity.

\section{Objective: Two Equivalent Views}
\label{sec:objective}

\subsection{Behavior-prior (default-policy) view}
Given a temperament-conditioned default policy $\pi_0^\tau(\cdot|s)$, we solve
\begin{equation}
\label{eq:behav}
\max_{\theta}\;\; \E\!\left[\sum_{t} r(s_t,a_t)\right]
-\alpha \,\E_{s\sim d^\pi} \KL\!\big(\pi_\theta(\cdot|s)\,\Vert\,\pi_0^\tau(\cdot|s)\big)
\quad \text{s.t.}\quad \E\!\Big[\sum_t c(s_t,a_t)\Big]\le \kappa,
\end{equation}
or the risk-sensitive variant that minimizes $\cvar_q(\sum_t c)$ with a Lagrangian penalty.

\subsection{Parameter-prior (PAC-Bayesian) view}
Let $p_\tau(\theta)$ be a temperament-conditioned prior over parameters and $q(\theta)$ a variational posterior. Optimize
\begin{equation}
\label{eq:param}
\max_{q}\;\; \E_{\theta\sim q}\Big[J(\theta)\Big] - \beta \,\KL\!\big(q(\theta)\,\Vert\,p_\tau(\theta)\big)
\quad \text{s.t.}\quad \E\!\Big[\sum_t c\Big]\le \kappa \;\;\text{or}\;\; \cvar_q\Big(\sum_t c\Big)\le \kappa_q.
\end{equation}

\begin{proposition}[Coupling the views via convexity]
\label{prop:couple}
Let $\pi_q(\cdot|s)\defeq \E_{\theta\sim q}[\pi_\theta(\cdot|s)]$. Then
$\E_s \KL\big(\pi_q(\cdot|s)\,\Vert\, \pi_0^\tau(\cdot|s)\big)\le
\E_{\theta\sim q}\E_s \KL\big(\pi_\theta(\cdot|s)\,\Vert\, \pi_0^\tau(\cdot|s)\big)$
by convexity of $\KL$ in its first argument. Under standard regularity, maximizing \eqref{eq:param} upper-bounds the behavior-regularized objective with $\alpha$ matched to $\beta$ and the induced $\pi_0^\tau$ given by marginalizing $p_\tau$.
\end{proposition}

\section{Temperament Priors That Compose}
\label{sec:priors}
We parameterize $p_\tau(\theta)$ as a tempered log-linear (exponential-family) prior:
\begin{equation}
\label{eq:expfam}
p_\tau(\theta) \propto \exp\!\Big(\tfrac{1}{T}\,\eta(\tau)^\top f(\theta)\Big), \quad
\eta(\tau)=W\tau + b,\;\; T\ge 1,
\end{equation}
with sufficient statistics $f(\theta)$ (e.g., policy logits/features), trait weights $\eta(\tau)$, and temperature $T$ to prevent over-confident collapse. We promote sparse trait influence via group penalties on rows of $W$.

\paragraph{Composability without brittleness.}
Traditional Product-of-Experts multiplies densities and can over-concentrate. The tempered log-linear form recovers PoE behavior as $T\to 0$ but remains conservative for $T\ge 1$; $T$ becomes a knob for exploration--regularization trade-offs.

\section{Mis-specification Escape: KL Budgets and Scheduling}
\label{sec:escape}
We track the empirical advantage of deviating from $\pi_0^\tau$:
$A^\mathrm{dev}_t \defeq \E_{s\sim d_t}\big[ \E_{a\sim \pi_\theta}[Q(s,a)] - \E_{a\sim \pi_0^\tau}[Q(s,a)] \big]$.
When $A^\mathrm{dev}_t$ is persistently positive, we relax prior strength by annealing $\alpha$ (behavior view) or $\beta$/$T$ (parameter view). Additionally, we impose a per-episode KL budget $\E_s \KL(\pi_\theta\,\Vert\,\pi_0^\tau)\le B$ with a dual variable updated by ascent, guaranteeing eventual escape from a mis-specified default.

\section{Theory: Complexity, Robustness, and Trade-offs}
\label{sec:theory}

\begin{assumption}[Bounded losses]
For a bounded surrogate loss $\ell(\theta)\in[0,1]$ estimating policy suboptimality or constraint violation on $n$ i.i.d.\ episodes, standard PAC-Bayesian conditions apply.
\end{assumption}

\begin{theorem}[PAC-Bayesian generalization with temperament priors]
\label{thm:pacbayes}
With probability at least $1-\delta$, for all posteriors $q$,
\[
\E[\ell(q)] \;\le\; \hat{\E}[\ell(q)] \;+\;
\sqrt{\frac{\KL\!\big(q\Vert p_\tau\big) + \ln \tfrac{1}{\delta}}{2n}}.
\]
Thus, informative temperament priors reduce the complexity term and the episodes required to achieve a target generalization gap.
\end{theorem}

\begin{proposition}[Metric-entropy corollary]
If restricting to the temperament-consistent subclass $\Pi_\tau$ reduces covering number $N_\tau(\varepsilon)\ll N(\varepsilon)$, then any PAC-style sample-complexity bound replacing $\ln N(\varepsilon)$ with $\ln N_\tau(\varepsilon)$ improves correspondingly.
\end{proposition}

\begin{proposition}[Pareto geometry via temperature]
\label{prop:pareto}
Let $T$ be the prior temperature in \eqref{eq:expfam}. Under smoothness, the local slope of the Pareto frontier between (a) expected time-to-regime and (b) $\cvar_q$ of cost is proportional to $\frac{\partial \alpha^\star}{\partial T}$ (behavior view) or $\frac{\partial \beta^\star}{\partial T}$ (parameter view); thus $T$ controls risk--speed trade-offs in a calibrated way.
\end{proposition}

\begin{remark}[Robustness under mis-specification]
When $p_\tau$ is wrong, the penalty depends on $\KL(q\Vert p_\tau)$; the KL budget and annealing bound regret inflation by limiting over-regularization duration.
\end{remark}

\section{Algorithm}
\label{sec:algorithm}

\begin{algorithm}[H]
\caption{TCPS: Temperament-KL Policy Gradient with Safety and KL Budget}
\label{alg:tcps}
\begin{algorithmic}[1]
\Require Encoder $\phi$, temperament route (probe/questionnaire/logs), prior map $(W,b,T)$, initial $\alpha$, KL budget $B$, CVaR level $q$, cost budget $\kappa$
\State Obtain $\tau\leftarrow$ route; set $\eta\!\leftarrow W\tau+b$; build $p_\tau(\theta)\propto \exp(\eta^\top f(\theta)/T)$ and/or $\pi_0^\tau$
\For{episodes $t=1\ldots T_{\max}$}
  \State Roll out $\pi_\theta$; estimate returns, costs; estimate $Q$ or advantages
  \State Compute loss $L = -\E[\text{return}] + \alpha\, \E_s \KL(\pi_\theta\,\Vert\,\pi_0^\tau) + \lambda\, \hat{\cvar}_q(\sum c-\kappa)_+$
  \State Update $\theta \leftarrow \theta - \eta_\theta \grad_\theta L$
  \State Measure empirical KL $K_t \leftarrow \E_s \KL(\pi_\theta\,\Vert\,\pi_0^\tau)$
  \State Update duals: $\lambda \leftarrow [\lambda + \eta_\lambda (\hat{\cvar}_q - \kappa_q)]_+$,\quad
   $\nu \leftarrow [\nu + \eta_\nu (K_t - B)]_+$
  \State Anneal $\alpha \leftarrow \alpha / (1+\rho A^\mathrm{dev}_t)$; optionally raise $T$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experimental Protocol}
\label{sec:experiments}

\paragraph{Environments.}
Three room families with distinct affordances, each with target/reject sets and safety costs; plus a \emph{hostile} variant where retaliation dynamics invert.

\paragraph{Temperament routes.}
Primary: probe episodes ($\leq 5$ scripted dilemmas). Sensitivity: questionnaire; logs.

\paragraph{Baselines.}
(1) Neutral KL to a generic default. (2) Behavior-regularized offline RL (e.g., TD3+BC-style) with same data budget. (3) Imitation-then-RL warm-start. (4) Meta-RL/hypernetwork conditioned on the same probe episodes.

\paragraph{Ablations.}
Factorial deconfounding: prior-only vs shaping-only vs exploration-only vs all. Trait ablations: zero-out each trait weight; measure $\Delta$ label-efficiency and $\Delta$ CVaR. Static vs slowly updated $\tau_t$ (EMA).

\paragraph{Metrics.}
Time-to-regime CDFs (Kaplan--Meier), cumulative regret with $95\%$ CI, $\cvar_q$ of cost, probability of budget breach, and KL drift to $\pi_0^\tau$. Report Pareto fronts (time vs CVaR).

\paragraph{Reporting.}
Medians, IQRs, $95\%$ CIs over $\ge 50$ seeds; fixed random seeds and configs released. No cherry-picked runs.

\section{Example Domain (Box)}
\label{sec:example}
\begin{example}[BA--FRC room family as CMDP]
Let $\phi(s,a) = [\text{self-halt},\, \text{bystander-halt},\, \text{social-standing},\, \text{retaliation}]$.
Temperament with \emph{low} self-halt cost, \emph{high} bystander-halt cost, \emph{high} social-standing weight, and retaliation tolerance $\lambda_r>0$ induces:
(i) higher prior mass on face-restoring options in $\pi_0^\tau$,
(ii) higher cost weight on bystander-halt, and
(iii) CVaR tolerance for retaliation. TCPS should reach target set $G$ notably faster than neutral KL with fewer tail-risk excursions.
\end{example}

\section{Discussion}
TCPS frames ``people priors'' as policy priors, but preserves principled escape when the prior is wrong. Temperament temperature $T$ and KL budget $B$ become explicit dials for the speed--risk frontier. The tempered log-linear construction composes traits without brittle products.

\section{Limitations}
Temperament inference may drift under strong distribution shift; trait encoders can entangle constructs; and CMDP costs depend on correct state labeling. We mitigate with slow $\tau_t$ updates, sparse trait weights, and ablations, but deployment requires careful feature audits.

\section{Conclusion}
Temperament-Conditioned Policy Search treats operator temperament as a first-class prior in entropy-regularized control. The result is cheaper, safer exploration that aligns with lived strategies, together with guarantees and escape routes.

\section*{Author Checklist (for reproducibility)}
\begin{itemize}[leftmargin=1.5em]
\item Pre-registered baselines, ablations, and metrics.
\item Released seeds, configs, and logs (train/val splits).
\item Reported medians, IQRs, and $95\%$ CIs; no single-run claims.
\item Mis-specification study with KL-budget escape curves.
\end{itemize}

\appendix

\section{Temperament Schema (YAML)}
\label{app:yaml}
\noindent The schema compiles to a tempered exponential-family prior (no raw PoE).
\begin{lstlisting}[language=YAML]
temperament:
  stop_friction:
    self: low
    bystanders: high
  social_standing_weight: 2.0
  retaliation_tolerance: 0.7   # influences CVaR level q or its multiplier

prior:
  family: exponential
  temperature: 1.5              # T >= 1 prevents over-confidence
  stats:
    - name: switch_penalty
      target: -0.8
      group: stop_self
    - name: bystander_cost_weight
      target: 1.5
      group: bystander_guard
    - name: option_mixture
      dirichlet: {apology: 3.0, warn: 2.0, escalate: 1.0}
regularization:
  sparsity: group_lasso         # trait weights W are group-sparse
  alpha_init: 0.5               # behavior-prior strength
  kl_budget: 0.2                # per-episode KL limit
safety:
  criterion: CVaR
  q: 0.10
\end{lstlisting}

\section{Proof Sketches}
\subsection*{Proof of Proposition~\ref{prop:couple}}
By convexity of $\KL(\cdot\Vert \pi_0^\tau)$ in its first argument,
$\KL(\E_q[\pi_\theta],\pi_0^\tau)\le \E_q[\KL(\pi_\theta,\pi_0^\tau)]$ pointwise in $s$.
Taking $\E_s$ preserves the inequality. Under mild conditions, matching regularization strengths yields equivalent optima up to this convexity gap.

\subsection*{Proof idea for Theorem~\ref{thm:pacbayes}}
Apply standard PAC-Bayesian bounds to bounded per-episode losses that upper-bound policy suboptimality or constraint violation. The key term $\KL(q\Vert p_\tau)$ captures how strongly the posterior deviates from the temperament prior; smaller KL implies tighter generalization.

\subsection*{Sketch for Proposition~\ref{prop:pareto}}
Consider the Lagrangian of \eqref{eq:behav} with duals for KL budget and CVaR. Envelope theorems imply that the derivative of the optimal value w.r.t.\ $T$ propagates through the optimal $\alpha^\star(T)$ or $\beta^\star(T)$. Since the KL and CVaR terms are the only $T$-coupled regularizers (via $p_\tau$), local Pareto slopes are proportional to $\partial \alpha^\star/\partial T$ or $\partial \beta^\star/\partial T$.

\section{Implementation Notes}
\begin{itemize}[leftmargin=1.5em]
\item \textbf{Default policy construction.} Build $\pi_0^\tau$ as an options mixture with logits set by $\eta(\tau)$; the behavior view then regularizes toward this mixture.
\item \textbf{Dynamic temperament.} Update $\tau_t \leftarrow (1-\zeta)\tau_{t-1} + \zeta\,\phi(\text{probe}_t)$ with small $\zeta$ to avoid oscillations.
\item \textbf{Risk estimators.} Use CVaR-on-the-fly via the infimum representation with differentiable $\eta$; stabilize with moving quantile estimates.
\end{itemize}

\section{Experimental Details}
\subsection*{Baselines}
Neutral KL; behavior-regularized offline RL with identical data budget; BC warm-start; meta-RL/hypernetwork from the same probe episodes.

\subsection*{Ablations}
Factorial separation of prior strength, shaping, and exploration bonus. Trait dropouts (set selected rows of $W$ to zero). Static vs EMA-updated $\tau_t$.

\subsection*{Reporting}
Median time-to-regime, Kaplan--Meier curves, cumulative regret with $95\%$ CIs, CVaR@0.1, budget-breach probability, and KL drift. Include Pareto fronts and sensitivity to temperature $T$ and KL budget $B$.

\end{document}